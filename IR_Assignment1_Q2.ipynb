{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment1_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####Samad\n",
        "####Shashwat"
      ],
      "metadata": {
        "id": "AHYt-yOSmlkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFetqQUkmiKh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = 'ira1.zip'\n",
        "with ZipFile('ira1.zip', 'r') as zip:\n",
        "  zip.extractall()\n",
        "print('Extraction Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwNDdoB0m0UR",
        "outputId": "3bfecce1-a8f4-4cba-ad7d-287580736335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# folders=os.listdir(r'C:\\Users\\shash\\Desktop\\ira1')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moVFV-SOm8Z0",
        "outputId": "534b2d9b-4c65-47d9-fe23-ce4e7665b145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ters(inp):\n",
        "    temp=(c for c in inp if not c.isdigit() and c.isalnum() and c not in string.punctuation)\n",
        "    temp1=''\n",
        "    return temp1.join(temp)"
      ],
      "metadata": {
        "id": "E91sKAbPm_Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pro(inp):\n",
        "    one=1\n",
        "    zero=0\n",
        "    stop=set(stopwords.words('english')) # makes a set of stopwords\n",
        "    wordt = word_tokenize(inp.lower())\n",
        "    # wordt = list(dict.fromkeys(wordt)) # We need duplicates\n",
        "    valid=[k for k in wordt if k not in stop]\n",
        "    valid=[ters(k) for k in valid]\n",
        "    for i in range(1,2):\n",
        "      one=one+zero\n",
        "    valid=[k for k in valid if len(k) > 1*one]\n",
        "    for i in range(1,2):\n",
        "      one=one+zero\n",
        "    return valid"
      ],
      "metadata": {
        "id": "-V9NO9wwnAvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readf(filep):\n",
        "    try:\n",
        "        with open(filep ,'r', encoding = \"utf-8\", errors = 'replace') as ff:\n",
        "            corpus = ff.read().replace('\\n', ' ')\n",
        "            one=1\n",
        "            zero=0\n",
        "            corpus = pro(corpus)\n",
        "            for i in range(1,2):\n",
        "              one=one+zero\n",
        "            return corpus\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "l3L0g9SenCxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "file_index={}\n",
        "pos_index={}\n",
        "i=1\n",
        "j=1\n",
        "k=1\n",
        "for folder in os.listdir('ira1'):\n",
        "    file_index[i]=folder\n",
        "    i=i+1\n",
        "path=\"ira1\"\n",
        "for folder in os.listdir('ira1'):\n",
        "    words=(readf(os.path.join(path,folder)))\n",
        "    k=1\n",
        "    for word in words:\n",
        "        if word in pos_index.keys():\n",
        "          if j in pos_index[word].keys():     # Document already present, just add to list\n",
        "            pos_index[word][j].append(k)\n",
        "          else:\n",
        "            pos_index[word][j] = list()       # Add an entry to pos_index[word] with key set as document id, and value as a new list\n",
        "            pos_index[word][j].append(k)\n",
        "        else:                                 # New word, Add an entry to pos_index\n",
        "          pos_index[word] = {}\n",
        "          pos_index[word][j] = list()\n",
        "          pos_index[word][j].append(k)\n",
        "        k += 1\n",
        "\n",
        "        # else:\n",
        "    j=j+1\n",
        "noOfDocuments = j-1"
      ],
      "metadata": {
        "id": "yokXTsamnl7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_index['potato'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgyw17m1pxgh",
        "outputId": "a23ad16a-dcb4-4ebc-e848-84125d623d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{6: [900, 2167], 44: [279], 72: [205], 103: [467, 646], 115: [15360, 15433, 16603], 149: [160], 165: [248], 183: [163], 226: [509, 957], 236: [99], 262: [942], 283: [1884], 308: [2102], 325: [2769], 327: [578], 375: [969], 388: [866, 1327], 407: [132, 268], 422: [120, 126, 1672, 1705, 1708, 1713, 1717, 1720, 1727, 1746, 2245, 2270, 2275, 2277, 2278, 3144, 3174, 3185, 3196, 3202], 429: [381], 443: [41, 42, 44], 508: [259, 1158], 509: [4050], 510: [26], 589: [105], 592: [4892], 600: [138], 605: [1802], 645: [480, 492, 509, 523, 534, 552, 565], 655: [51, 109], 663: [193], 706: [79, 266], 711: [1802], 750: [1227], 754: [249], 761: [196], 781: [2015], 830: [240], 843: [159, 549, 923, 946, 954], 880: [350], 893: [142, 168, 201], 909: [2239], 918: [47], 933: [12280, 12288, 12312, 13372, 13396, 20896, 20918, 20970, 20984, 21008, 33694], 954: [90], 962: [16, 59, 161, 281, 404, 1323, 1471, 2010, 2387, 2633, 2640, 2721, 2915, 3038, 3098, 3108, 3217, 3224, 3292, 3617, 3635, 3682, 3740, 4665, 5788, 5796, 5818, 5826, 7411, 7420, 7606, 7624, 7691, 7699, 8236, 8238, 8265], 988: [448], 1006: [1825, 1883, 4210], 1030: [220, 233, 2488, 4773, 6131, 6140, 6255, 7905], 1069: [448], 1093: [399, 785], 1121: [106]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt=int(input(\"Enter number of queries: \"))\n",
        "\n",
        "while (cnt>0):\n",
        "  phrase=input(\"Enter Phrase: \")\n",
        "  phrase = pro(phrase)                    # preprocess the input phrase\n",
        "  print(\"Processed phrase: \", phrase)\n",
        "  noWords = len(phrase)\n",
        "  ans = []\n",
        "  flag = True\n",
        "  for j in pos_index[phrase[0]]:          # Loop across all documents which contain the first word of phrase\n",
        "    posList = pos_index[phrase[0]][j]     # List of all positions of word in j'th document\n",
        "    flag = True\n",
        "    for pos in posList:                   # Loop for all possible starting positions\n",
        "      k=1\n",
        "      for nextWord in range(1,len(phrase)):\n",
        "        if (j in pos_index[phrase[nextWord]] and ( (pos+k) in pos_index[phrase[nextWord]][j]) ):\n",
        "          # Corresponding word in phrase found at correct position with respect to the first word\n",
        "          garbage = 0\n",
        "        else:\n",
        "          # Corresponding word not found in correct position, phrase not present for this value of pos\n",
        "          flag = False\n",
        "        k += 1\n",
        "      if (flag):\n",
        "        # All words in phrase were found in correct order\n",
        "        ans.append(j)\n",
        "  print(\"Phrase found in\", len(ans), \" documents\")\n",
        "  print(\"Phrase found in following documents: \", ans)\n",
        "  cnt -= 1\n"
      ],
      "metadata": {
        "id": "McBX29sJsZzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c1a3ca-9a88-4341-f040-b703bbf5ad19"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of queries: 2\n",
            "Enter Phrase: good morning\n",
            "Processed phrase:  ['good', 'morning']\n",
            "Phrase found in 8  documents\n",
            "Phrase found in following documents:  [229, 239, 431, 481, 706, 791, 871, 871]\n",
            "Enter Phrase: it is a sweet potato\n",
            "Processed phrase:  ['sweet', 'potato']\n",
            "Phrase found in 2  documents\n",
            "Phrase found in following documents:  [605, 711]\n"
          ]
        }
      ]
    }
  ]
}